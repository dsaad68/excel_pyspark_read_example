{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27b7b8e0-829d-4b93-b93e-226a4ab629cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Daniels-PC:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Python Spark SQL basic example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b8853742b0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import input_file_name, monotonically_increasing_id, split, col, isnan, when, count\n",
    "from pyspark.sql.types import StringType, StructType, DoubleType\n",
    "\n",
    "# Assembly or download spark-excel and its dependencies\n",
    "jars = [\n",
    "    \"C:\\\\Users\\\\Dsaad\\\\GitHub\\\\pyspark\\\\jars\\\\spark-hadoopoffice-ds_2.12-1.6.3.jar\"    \n",
    "]\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.jars\", \",\".join(jars)) \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4a2d6df-8a9b-421b-9d3a-59865f7505fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|         productGUID|       SKU|\n",
      "+--------------------+----------+\n",
      "|        Product_GUID|       SKU|\n",
      "|51D0242F-ADCC-4B7...|2608580315|\n",
      "|B18557B0-E0FD-4A3...|   AT2550A|\n",
      "|0A4DC340-167F-44B...|2609200213|\n",
      "|69D084D2-1297-438...|2607224392|\n",
      "|0820E32F-06A6-460...|2608608H66|\n",
      "|45A90BED-BA15-4A9...|0600882174|\n",
      "|45A90BED-BA15-4A9...|06008A6271|\n",
      "|91765661-8B3C-4F1...|06019B2901|\n",
      "|91765661-8B3C-4F1...|06019B2902|\n",
      "+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- productGUID: string (nullable = true)\n",
      " |-- SKU: string (nullable = true)\n",
      "\n",
      "+-----------+---+\n",
      "|productGUID|SKU|\n",
      "+-----------+---+\n",
      "|          0|  0|\n",
      "+-----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create a schema to read the excel file\n",
    "schema = StructType() \\\n",
    "        .add(\"c0\",StringType(),True) \\\n",
    "        .add(\"c1\",StringType(),True) \\\n",
    "        .add(\"c2\",StringType(),True)\n",
    "\n",
    "# read the excel file to fact DataFrame with custom schema\n",
    "fact = spark.read.format('org.zuinnote.spark.office.excel')\\\n",
    "            .option(\"read.spark.simpleMode\",True) \\\n",
    "            .schema(schema) \\\n",
    "            .load(\"./data/skuToGUID.xlsx\")\n",
    "\n",
    "fact = fact.drop(col('c0'))\n",
    "\n",
    "# dropping all the Null and None values\n",
    "fact = fact.na.drop(how='any' , subset=[\"c1\",\"c2\"])\n",
    "\n",
    "# renaming columns\n",
    "fact = fact.withColumnRenamed(\"c1\",\"productGUID\") \\\n",
    "           .withColumnRenamed(\"c2\",\"SKU\") \\\n",
    "           .withColumn(\"SKU\",col(\"SKU\").cast(StringType())) \\\n",
    "           .withColumn(\"productGUID\",col(\"productGUID\").cast(StringType()))\n",
    "\n",
    "fact.show(10)\n",
    "\n",
    "fact.printSchema()\n",
    "\n",
    "# counting nulls\n",
    "fact.select( [ count( when(isnan(c) | col(c).isNull(), c) ).alias(c) for c in fact.columns] ).show(truncate = True)\n",
    "\n",
    "# create a temp view\n",
    "fact.createOrReplaceTempView(\"fact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5811877b-49b4-4715-a85a-225bd636d2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------+--------------------+----------+--------------------+-----+--------+\n",
      "|RowID|productGroupID|    productGroupName|materialID|        materialName|sales|quantity|\n",
      "+-----+--------------+--------------------+----------+--------------------+-----+--------+\n",
      "|    5|           005|Impact Drills < 500W|0603130004|      EasyImpact 550| 37.0|     1.0|\n",
      "|    6|           005|Impact Drills < 500W|0603130004|      EasyImpact 550| 26.0|     1.0|\n",
      "|    7|           005|Impact Drills < 500W|0603130020|      EasyImpact 550|159.0|     5.0|\n",
      "|    8|           009|Rotary Hammers 1-2KG|06033A9302|PBH 2100 RE + drills|106.0|     2.0|\n",
      "|    9|           009|Rotary Hammers 1-2KG|06033A9303|   PBH 2100 RE + set| 56.0|     1.0|\n",
      "|   10|           009|Rotary Hammers 1-2KG|06033A9320|         PBH 2100 RE|233.0|     4.0|\n",
      "|   11|           017|       Drill Drivers|060397290X|      EasyDrill 12-2|268.0|     4.0|\n",
      "|   12|           017|       Drill Drivers|06039A210B|      EasyDrill 1200|110.0|     2.0|\n",
      "|   13|           017|       Drill Drivers|06039A3100|       PSR 1800 LI-2| 94.0|     2.0|\n",
      "|   14|           017|       Drill Drivers|06039A3101|       PSR 1800 LI-2|155.0|     2.0|\n",
      "+-----+--------------+--------------------+----------+--------------------+-----+--------+\n",
      "only showing top 10 rows\n",
      "\n",
      "root\n",
      " |-- RowID: long (nullable = false)\n",
      " |-- productGroupID: string (nullable = true)\n",
      " |-- productGroupName: string (nullable = true)\n",
      " |-- materialID: string (nullable = true)\n",
      " |-- materialName: string (nullable = true)\n",
      " |-- sales: double (nullable = true)\n",
      " |-- quantity: double (nullable = true)\n",
      "\n",
      "+-----+--------------+----------------+----------+------------+-----+--------+\n",
      "|RowID|productGroupID|productGroupName|materialID|materialName|sales|quantity|\n",
      "+-----+--------------+----------------+----------+------------+-----+--------+\n",
      "|    0|             0|               0|         0|           0|    0|       0|\n",
      "+-----+--------------+----------------+----------+------------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# reading the 31 excel file containg the sales data\n",
    "df2 = spark.read.format('org.zuinnote.spark.office.excel')\\\n",
    "    .option(\"read.spark.simpleMode\", True) \\\n",
    "    .option(\"columnNameOfRowNumber\", \"RowID\") \\\n",
    "    .load(\"./data/sales/\")\n",
    "\n",
    "# adding a RowID\n",
    "df2 = df2.withColumn(\"RowID\", monotonically_increasing_id())\n",
    "\n",
    "# selecting the needed the columns\n",
    "df2 = df2.select([\"RowID\",'c5','c6','c7','c8','c14','c15'])\n",
    "\n",
    "# filtering the header\n",
    "df2 = df2.filter( ( df2['c5'] != '' ) & ( df2['c6'] != '' ) & ( df2['c7'] != '' ) & ( df2['c8'] != '' ) )\n",
    "\n",
    "# Dropping all the Null and None values\n",
    "df2 = df2.na.drop(how='any' , subset=[\"c5\",\"c6\",\"c7\",\"c8\",\"c14\",\"c15\"])\n",
    "\n",
    "# cleaning the quantity column and removing 'PCE'\n",
    "df2 = df2.withColumn('quantity', split(df2['c14'], ' ').getItem(0))\n",
    "df2 = df2.drop(col('c14'))\n",
    "\n",
    "# renaming columns and casting types\n",
    "df2 = df2.withColumnRenamed(\"c5\",\"productGroupID\") \\\n",
    "         .withColumnRenamed(\"c6\",\"productGroupName\") \\\n",
    "         .withColumnRenamed(\"c7\",\"materialID\") \\\n",
    "         .withColumnRenamed(\"c8\",\"materialName\") \\\n",
    "         .withColumnRenamed(\"c15\",\"sales\") \\\n",
    "         .withColumn(\"sales\", col(\"sales\").cast(DoubleType())) \\\n",
    "         .withColumn(\"quantity\", col(\"quantity\").cast(DoubleType()))\n",
    "\n",
    "# Dropping all the Null and None values\n",
    "df2 = df2.na.drop(how='any' , subset=[\"sales\",\"quantity\"])\n",
    "\n",
    "# filtering the negative values\n",
    "df2 = df2.filter( ( df2['sales'] >= 0 ) & ( df2['quantity'] >= 0 ) )\n",
    "\n",
    "# remove all data which is attributed to the Product Group 175\n",
    "sales = df2.filter( df2['productGroupID'] != 175) \n",
    "\n",
    "# showing the result\n",
    "sales.show(10)\n",
    "\n",
    "sales.printSchema()\n",
    "\n",
    "# counting nulls\n",
    "sales.select( [ count( when(isnan(c) | col(c).isNull(), c) ).alias(c) for c in sales.columns] ).show()\n",
    "\n",
    "# create a temp view\n",
    "sales.createOrReplaceTempView(\"sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bf2b80a9-b210-4407-91d0-89526b23910a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------+----------+\n",
      "|         productGUID|       materialName|sumOfQuantity|sumOfSales|\n",
      "+--------------------+-------------------+-------------+----------+\n",
      "|5324F254-9A9C-475...|          PKS 66 AF|        181.0|   21883.0|\n",
      "|99571ACA-CB1C-4B9...|     EasyImpact 500|       1062.0|   30766.0|\n",
      "|DF3A1BA5-1FDC-474...|           ART 27 +|         37.0|    1887.0|\n",
      "|9B0EE701-B7BF-4E3...|      PSB 1800 Li-2|        114.0|    8335.0|\n",
      "|94B8DC22-EA8D-4F4...|          AXT 25 TC|        163.0|   53870.0|\n",
      "|3F940372-813C-416...|            PTC 470|         48.0|    4272.0|\n",
      "|7A0531CD-257B-475...|  AdvancedRotak 770|         19.0|    3746.0|\n",
      "|86313E77-BE96-41D...|          ART 23-28|          5.0|      90.0|\n",
      "|B671FB55-CEAB-42F...|    UniversalVac 18|        726.0|   32296.0|\n",
      "|2A1C04BA-43D2-4F1...|EasyHedgeCut 12-450|         16.0|    1070.0|\n",
      "+--------------------+-------------------+-------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('''SELECT fact.productGUID\n",
    "                   ,sales.materialName\n",
    "                   ,sum(sales.quantity) as sumOfQuantity\n",
    "                   ,sum(sales.sales) as sumOfSales\n",
    "                FROM sales\n",
    "                JOIN fact\n",
    "                ON sales.materialID = fact.SKU\n",
    "                GROUP BY fact.productGUID, sales.materialName\n",
    "            ''').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44730556-0101-470a-87b3-780de101f312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------+----------+\n",
      "|         productGUID|       materialName|sum(quantity)|sum(sales)|\n",
      "+--------------------+-------------------+-------------+----------+\n",
      "|5324F254-9A9C-475...|          PKS 66 AF|        181.0|   21883.0|\n",
      "|99571ACA-CB1C-4B9...|     EasyImpact 500|       1062.0|   30766.0|\n",
      "|DF3A1BA5-1FDC-474...|           ART 27 +|         37.0|    1887.0|\n",
      "|9B0EE701-B7BF-4E3...|      PSB 1800 Li-2|        114.0|    8335.0|\n",
      "|94B8DC22-EA8D-4F4...|          AXT 25 TC|        163.0|   53870.0|\n",
      "|3F940372-813C-416...|            PTC 470|         48.0|    4272.0|\n",
      "|7A0531CD-257B-475...|  AdvancedRotak 770|         19.0|    3746.0|\n",
      "|86313E77-BE96-41D...|          ART 23-28|          5.0|      90.0|\n",
      "|B671FB55-CEAB-42F...|    UniversalVac 18|        726.0|   32296.0|\n",
      "|2A1C04BA-43D2-4F1...|EasyHedgeCut 12-450|         16.0|    1070.0|\n",
      "|A37AFACE-3433-40C...|       AHS 48-20 LI|        106.0|   11014.0|\n",
      "|83797DF3-15FF-4F2...|     AXT RAPID 2000|          4.0|     594.0|\n",
      "|269F4AD8-23F7-495...|             ARM 37|        227.0|   24595.0|\n",
      "|58176BFC-30FD-441...|         PEX 300 AE|        889.0|   59130.0|\n",
      "|1252AE1F-FCC2-492...|     IXO Collection|         37.0|     538.0|\n",
      "|F24C8265-86B9-4B6...|     IXO Collection|        696.0|    5411.0|\n",
      "|39F8928C-74DB-4B3...|   ART combitrim 23|          5.0|      78.0|\n",
      "|FF735853-2639-471...|             AKE 40|         34.0|    2501.0|\n",
      "|D3CF3EEC-4E80-4D3...|      Sanding Sheet|        251.0|     886.0|\n",
      "|25836A5A-E705-49C...| Large Repl. Cloths|        278.0|    1882.0|\n",
      "+--------------------+-------------------+-------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sales.join(fact, sales.materialID == fact.SKU, \"inner\") \\\n",
    "     .select(['productGUID' ,'materialName', 'quantity' ,'sales']) \\\n",
    "     .groupBy(\"productGUID\",\"materialName\") \\\n",
    "     .sum(\"quantity\",\"sales\") \\\n",
    "     .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaa9f78-fbb8-4f79-a2c4-f0a9a8785ef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
